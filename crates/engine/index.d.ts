/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

/** Word with timing information (for word-level timestamps) */
export interface Word {
  /** The word text */
  word: string
  /** Start time in seconds */
  start: number
  /** End time in seconds */
  end: number
  /** Word probability/confidence (0.0 to 1.0) */
  probability: number
}
/** Transcription segment with timing and confidence information */
export interface Segment {
  /** Segment ID (0-indexed) */
  id: number
  /** Seek position in audio frames */
  seek: number
  /** Start time in seconds */
  start: number
  /** End time in seconds */
  end: number
  /** Transcribed text */
  text: string
  /** Token IDs */
  tokens: Array<number>
  /** Decoding temperature used */
  temperature: number
  /** Average log probability */
  avgLogprob: number
  /** Compression ratio */
  compressionRatio: number
  /** Probability of no speech */
  noSpeechProb: number
  /** Word-level timestamps (if wordTimestamps option was enabled) */
  words?: Array<Word>
}
/** Voice Activity Detection (VAD) options */
export interface VadOptions {
  /** Speech detection threshold (0.0 to 1.0, default: 0.5) */
  threshold?: number
  /** Minimum speech duration in milliseconds (default: 250) */
  minSpeechDurationMs?: number
  /** Maximum speech duration in seconds (default: 30) */
  maxSpeechDurationS?: number
  /** Minimum silence duration in milliseconds to split segments (default: 2000) */
  minSilenceDurationMs?: number
  /** Analysis window size in milliseconds (default: 30) */
  windowSizeMs?: number
  /** Padding around speech segments in milliseconds (default: 400) */
  speechPadMs?: number
}
/** Transcription options */
export interface TranscribeOptions {
  /** Source language (e.g., "en", "de", "fr"). If not set, language is auto-detected. */
  language?: string
  /** Task to perform: "transcribe" or "translate" */
  task?: string
  /** Beam size for beam search (default: 5, set to 1 for greedy search) */
  beamSize?: number
  /** Beam search patience factor (default: 1.0) */
  patience?: number
  /** Exponential penalty applied to length during beam search (default: 1.0) */
  lengthPenalty?: number
  /** Penalty for repetition (default: 1.0, set > 1 to penalize) */
  repetitionPenalty?: number
  /** Prevent repetitions of ngrams with this size (default: 0, disabled) */
  noRepeatNgramSize?: number
  /** Sampling temperature (default: 1.0) */
  temperature?: number
  /** Suppress blank outputs at beginning (default: true) */
  suppressBlank?: boolean
  /** Maximum generation length (default: 448) */
  maxLength?: number
  /** Include word-level timestamps (default: false) */
  wordTimestamps?: boolean
  /** Initial prompt to provide context */
  initialPrompt?: string
  /** Prefix for the first segment */
  prefix?: string
  /** Suppress tokens (comma-separated IDs or special tokens) */
  suppressTokens?: string
  /** Apply condition on previous text (default: true) */
  conditionOnPreviousText?: boolean
  /** Compression ratio threshold for detecting failed decodings */
  compressionRatioThreshold?: number
  /** Log probability threshold for detecting failed decodings */
  logProbThreshold?: number
  /** No speech probability threshold */
  noSpeechThreshold?: number
  /** Enable Voice Activity Detection to filter out silent portions (default: false) */
  vadFilter?: boolean
  /** VAD-specific options */
  vadOptions?: VadOptions
  /**
   * Hallucination silence threshold in seconds (default: None)
   * If a segment's duration per word exceeds this, it's likely a hallucination
   */
  hallucinationSilenceThreshold?: number
}
/** Options for model loading */
export interface ModelOptions {
  /** Device to use: "cpu", "cuda", "metal", or "auto" (default: "auto") */
  device?: string
  /** Compute type (not used in whisper-rs, kept for API compatibility) */
  computeType?: string
  /** Number of CPU threads (0 = auto) */
  cpuThreads?: number
  /** GPU device index (for multi-GPU systems) */
  deviceIndex?: number
}
/** Transcription result with full segment data */
export interface TranscriptionResult {
  /** All transcription segments */
  segments: Array<Segment>
  /** Detected or specified language */
  language: string
  /** Language detection probability */
  languageProbability: number
  /** Total audio duration in seconds */
  duration: number
  /** Duration after VAD filtering (if enabled) */
  durationAfterVad: number
  /** Full transcription text */
  text: string
}
/** Language detection result */
export interface LanguageDetectionResult {
  /** Detected language code */
  language: string
  /** Detection probability */
  probability: number
}
/** Batch transcription result for a single file */
export interface BatchTranscriptionItem {
  /** Original file path */
  filePath: string
  /** Transcription result (None if error) */
  result?: TranscriptionResult
  /** Error message (None if success) */
  error?: string
  /** Current file index */
  currentIndex: number
}
/** Get list of supported model size aliases */
export declare function availableModels(): Array<string>
/** Check if a model is downloaded */
export declare function isModelAvailable(size: string): boolean
/** Get the path where a model would be stored */
export declare function getModelPath(size: string): string
/** Get the default cache directory for models */
export declare function getCacheDir(): string
/**
 * Download a model (async)
 * Returns the path to the downloaded model
 */
export declare function downloadModel(size: string, cacheDir?: string | undefined | null): Promise<string>
/** Decode audio file to raw samples (16kHz mono Float32) */
export declare function decodeAudio(path: string): Array<number>
/** Decode audio buffer to raw samples (16kHz mono Float32) */
export declare function decodeAudioBuffer(buffer: Buffer): Array<number>
/** Format seconds to timestamp string (HH:MM:SS.mmm or MM:SS.mmm) */
export declare function formatTimestamp(seconds: number, alwaysIncludeHours?: boolean | undefined | null): string
/** Check if GPU acceleration is available (Metal on macOS, CUDA on Linux/Windows) */
export declare function isGpuAvailable(): boolean
/** Get the number of available GPU devices */
export declare function getGpuCount(): number
/** Get the best available device ("metal", "cuda", or "cpu") */
export declare function getBestDevice(): string
/** A streaming transcription segment (stable or preview) */
export interface StreamingSegment {
  /** Segment text */
  text: string
  /** Start time in the audio stream (seconds) */
  start: number
  /** End time in the audio stream (seconds)  */
  end: number
  /** Whether this segment is final (won't change) or preview (may change) */
  isFinal: boolean
}
/** Result from processing streaming audio */
export interface StreamingResult {
  /** Stable (final) segments that won't change */
  stableSegments: Array<StreamingSegment>
  /** Preview text that may change with more audio */
  previewText?: string
  /** Current buffer duration in seconds */
  bufferDuration: number
  /** Total audio processed so far in seconds */
  totalDuration: number
}
/** Configuration for streaming transcription */
export interface StreamingOptions {
  /** Minimum buffer before transcription (seconds, default: 1.0) */
  minBufferSeconds?: number
  /** Stability margin from buffer end (seconds, default: 1.5) */
  stabilityMarginSeconds?: number
  /** Context overlap to keep after committing (seconds, default: 0.5) */
  contextOverlapSeconds?: number
  /** Maximum buffer size (seconds, default: 30.0) */
  maxBufferSeconds?: number
  /** Language for transcription */
  language?: string
  /** Beam size (default: 5) */
  beamSize?: number
}
export declare class Engine {
  /**
   * Create a new transcription engine from a model path or size
   *
   * # Arguments
   * * `model_path` - Either a path to a GGML model file, or a model size
   *                  alias ("tiny", "base", "small", "medium", "large-v2", "large-v3")
   */
  constructor(modelPath: string)
  /** Create a new transcription engine with options */
  static withOptions(modelPath: string, options?: ModelOptions | undefined | null): Engine
  /** Transcribe audio file (supports WAV, MP3, FLAC, OGG, M4A) */
  transcribeFile(audioPath: string, options?: TranscribeOptions | undefined | null): TranscriptionResult
  /** Legacy: transcribe from WAV file path, returns structured segments */
  transcribeSegments(audioPath: string, options?: TranscribeOptions | undefined | null): TranscriptionResult
  /** Simple transcription returning just the text (backward compatible) */
  transcribe(audioFile: string): string
  /** Transcribe with options, returning just the text */
  transcribeWithOptions(audioFile: string, options: TranscribeOptions): string
  /** Transcribe from a Buffer containing audio data (any supported format) */
  transcribeBuffer(buffer: Buffer, options?: TranscribeOptions | undefined | null): TranscriptionResult
  /** Transcribe from raw Float32Array samples (must be 16kHz mono, normalized to [-1, 1]) */
  transcribeSamples(samples: Array<number>, options?: TranscribeOptions | undefined | null): TranscriptionResult
  /**
   * Detect the language of audio
   * Note: This performs a quick transcription to detect language.
   * For efficiency, only the first 30 seconds are analyzed.
   */
  detectLanguage(audioPath: string): LanguageDetectionResult
  /** Detect language from buffer */
  detectLanguageBuffer(buffer: Buffer): LanguageDetectionResult
  /** Get the expected sampling rate (16000 Hz for Whisper) */
  samplingRate(): number
  /** Check if the model is multilingual */
  isMultilingual(): boolean
  /** Get the number of supported languages */
  numLanguages(): number
}
/**
 * Streaming transcription engine with LocalAgreement algorithm
 *
 * This enables true streaming transcription by:
 * 1. Maintaining a rolling audio buffer per session
 * 2. Running inference on overlapping windows
 * 3. Only emitting text that is "stable" (agreed upon across inference runs)
 */
export declare class StreamingEngine {
  /** Create a new streaming transcription engine */
  constructor(modelPath: string)
  /** Create a new streaming transcription engine with options */
  static withOptions(modelPath: string, options?: ModelOptions | undefined | null): StreamingEngine
  /**
   * Create a new streaming session
   * Returns the session ID
   */
  createSession(options?: StreamingOptions | undefined | null): number
  /**
   * Add audio samples to a streaming session and process
   *
   * Returns stable segments (final) and preview text (may change)
   */
  processAudio(sessionId: number, samples: Array<number>): StreamingResult
  /** Flush session - return all remaining audio as final */
  flushSession(sessionId: number): StreamingResult
  /** Reset a streaming session (clear buffer, keep session) */
  resetSession(sessionId: number): void
  /** Close a streaming session */
  closeSession(sessionId: number): void
  /** Get the number of active sessions */
  sessionCount(): number
  /** Get the expected sampling rate (16000 Hz for Whisper) */
  samplingRate(): number
}
