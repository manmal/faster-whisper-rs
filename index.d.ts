/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

/** Word with timing information (for word-level timestamps) */
export interface Word {
  /** The word text */
  word: string
  /** Start time in seconds */
  start: number
  /** End time in seconds */
  end: number
  /** Word probability/confidence (0.0 to 1.0) */
  probability: number
}
/** Transcription segment with timing and confidence information */
export interface Segment {
  /** Segment ID (0-indexed) */
  id: number
  /** Seek position in audio frames */
  seek: number
  /** Start time in seconds */
  start: number
  /** End time in seconds */
  end: number
  /** Transcribed text */
  text: string
  /** Token IDs */
  tokens: Array<number>
  /** Decoding temperature used */
  temperature: number
  /** Average log probability */
  avgLogprob: number
  /** Compression ratio */
  compressionRatio: number
  /** Probability of no speech */
  noSpeechProb: number
  /** Word-level timestamps (if wordTimestamps option was enabled) */
  words?: Array<Word>
}
/** Voice Activity Detection (VAD) options */
export interface VadOptions {
  /** Speech detection threshold (0.0 to 1.0, default: 0.5) */
  threshold?: number
  /** Minimum speech duration in milliseconds (default: 250) */
  minSpeechDurationMs?: number
  /** Maximum speech duration in seconds (default: 30) */
  maxSpeechDurationS?: number
  /** Minimum silence duration in milliseconds to split segments (default: 2000) */
  minSilenceDurationMs?: number
  /** Analysis window size in milliseconds (default: 30) */
  windowSizeMs?: number
  /** Padding around speech segments in milliseconds (default: 400) */
  speechPadMs?: number
}
/** Transcription options */
export interface TranscribeOptions {
  /** Source language (e.g., "en", "de", "fr"). If not set, language is auto-detected. */
  language?: string
  /** Task to perform: "transcribe" or "translate" */
  task?: string
  /** Beam size for beam search (default: 5, set to 1 for greedy search) */
  beamSize?: number
  /** Beam search patience factor (default: 1.0) */
  patience?: number
  /** Exponential penalty applied to length during beam search (default: 1.0) */
  lengthPenalty?: number
  /** Penalty for repetition (default: 1.0, set > 1 to penalize) */
  repetitionPenalty?: number
  /** Prevent repetitions of ngrams with this size (default: 0, disabled) */
  noRepeatNgramSize?: number
  /** Sampling temperature (default: 1.0) */
  temperature?: number
  /** Suppress blank outputs at beginning (default: true) */
  suppressBlank?: boolean
  /** Maximum generation length (default: 448) */
  maxLength?: number
  /** Include word-level timestamps (default: false) */
  wordTimestamps?: boolean
  /** Initial prompt to provide context */
  initialPrompt?: string
  /** Prefix for the first segment */
  prefix?: string
  /** Suppress tokens (comma-separated IDs or special tokens) */
  suppressTokens?: string
  /** Apply condition on previous text (default: true) */
  conditionOnPreviousText?: boolean
  /** Compression ratio threshold for detecting failed decodings */
  compressionRatioThreshold?: number
  /** Log probability threshold for detecting failed decodings */
  logProbThreshold?: number
  /** No speech probability threshold */
  noSpeechThreshold?: number
  /** Enable Voice Activity Detection to filter out silent portions (default: false) */
  vadFilter?: boolean
  /** VAD configuration options */
  vadOptions?: VadOptions
  /**
   * Hallucination silence threshold in seconds (default: None)
   * Segments with a silent duration longer than this will be considered hallucinations
   */
  hallucinationSilenceThreshold?: number
}
/** Model configuration options */
export interface ModelOptions {
  /** Device to use: "cpu" or "cuda" (default: "cpu") */
  device?: string
  /** Compute type: "default", "auto", "int8", "int8_float16", "int16", "float16", "float32" */
  computeType?: string
  /** Number of CPU threads per replica (0 for auto) */
  cpuThreads?: number
  /** Custom cache directory for auto-downloaded models */
  cacheDir?: string
}
/** Transcription result containing all segments and metadata */
export interface TranscriptionResult {
  /** All transcribed segments */
  segments: Array<Segment>
  /** Detected or specified language */
  language: string
  /** Language detection probability (0 if language was specified) */
  languageProbability: number
  /** Total audio duration in seconds */
  duration: number
  /** Audio duration after VAD filtering (equals duration if VAD not used) */
  durationAfterVad: number
  /** Full transcribed text (all segments joined) */
  text: string
}
/** Language detection result */
export interface LanguageDetectionResult {
  /** Detected language code */
  language: string
  /** Detection probability */
  probability: number
}
/** Download progress information */
export interface DownloadProgress {
  /** Current progress percentage (0-100) */
  percent: number
  /** Current file being downloaded */
  currentFile: string
  /** Total files to download */
  totalFiles: number
  /** Current file index */
  currentIndex: number
}
/** Get list of supported model size aliases */
export declare function availableModels(): Array<string>
/** Check if a model is downloaded */
export declare function isModelAvailable(size: string): boolean
/** Get the path where a model would be stored */
export declare function getModelPath(size: string): string
/** Get the default cache directory for models */
export declare function getCacheDir(): string
/**
 * Download a model (async)
 * Returns the path to the downloaded model
 */
export declare function downloadModel(size: string, cacheDir?: string | undefined | null): Promise<string>
/** Decode audio file to raw samples (16kHz mono Float32) */
export declare function decodeAudio(path: string): Array<number>
/** Decode audio buffer to raw samples (16kHz mono Float32) */
export declare function decodeAudioBuffer(buffer: Buffer): Array<number>
/** Format seconds to timestamp string (HH:MM:SS.mmm or MM:SS.mmm) */
export declare function formatTimestamp(seconds: number, alwaysIncludeHours?: boolean | undefined | null): string
/** Check if CUDA (GPU acceleration) is available */
export declare function isGpuAvailable(): boolean
/** Get the number of available CUDA GPU devices */
export declare function getGpuCount(): number
/** Get the best available device ("cuda" if GPU available, otherwise "cpu") */
export declare function getBestDevice(): string
export declare class Engine {
  /**
   * Create a new transcription engine from a model path or size
   *
   * # Arguments
   * * `model_path` - Either a path to a CTranslate2 model directory, or a model size
   *                  alias ("tiny", "base", "small", "medium", "large-v2", "large-v3")
   */
  constructor(modelPath: string)
  /** Create a new transcription engine with options */
  static withOptions(modelPath: string, options?: ModelOptions | undefined | null): Engine
  /** Transcribe audio file (supports WAV, MP3, FLAC, OGG, M4A) */
  transcribeFile(audioPath: string, options?: TranscribeOptions | undefined | null): TranscriptionResult
  /** Legacy: transcribe from WAV file path, returns structured segments */
  transcribeSegments(audioPath: string, options?: TranscribeOptions | undefined | null): TranscriptionResult
  /** Simple transcription returning just the text (backward compatible) */
  transcribe(audioFile: string): string
  /** Transcribe with options, returning just the text */
  transcribeWithOptions(audioFile: string, options: TranscribeOptions): string
  /** Transcribe from a Buffer containing audio data (any supported format) */
  transcribeBuffer(buffer: Buffer, options?: TranscribeOptions | undefined | null): TranscriptionResult
  /** Transcribe from raw Float32Array samples (must be 16kHz mono, normalized to [-1, 1]) */
  transcribeSamples(samples: Array<number>, options?: TranscribeOptions | undefined | null): TranscriptionResult
  /**
   * Detect the language of audio
   * Note: This performs a quick transcription to detect language.
   * For efficiency, only the first 30 seconds are analyzed.
   */
  detectLanguage(audioPath: string): LanguageDetectionResult
  /** Detect language from buffer */
  detectLanguageBuffer(buffer: Buffer): LanguageDetectionResult
  /** Get the expected sampling rate (16000 Hz for Whisper) */
  samplingRate(): number
  /** Check if the model is multilingual */
  isMultilingual(): boolean
  /** Get the number of supported languages */
  numLanguages(): number
}
